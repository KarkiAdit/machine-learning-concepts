{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Preprocessing Tools</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Data preprocessing is the process of cleaning and transforming raw data into a usable format for machine learning models. It involves handling missing values, scaling numerical data, encoding categorical data, and removing any inconsistencies. Proper preprocessing helps improve the accuracy and efficiency of machine learning models by ensuring they can interpret the data correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Importing the libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Importing the dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data.csv\")\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1:].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Taking care of the missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# Replace all the numerical null values with the column mean\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> One-hot encoding turns categories into separate columns, where each column has a value of 1 or 0. This way, each category is treated equally, without any order or ranking. For example, if we have a \"Color\" column with values \"Red,\" \"Blue,\" and \"Green,\" one-hot encoding would create three columns: \"Color_Red,\" \"Color_Blue,\" and \"Color_Green.\" If the original color was \"Red,\" the encoded values would be 1 in \"Color_Red\" and 0 in the other two columns, preventing the model from thinking one color is more important than another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the independent variable\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Encoding the dependent variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Do we have to apply feature scaling before or after splitting the data into training and the test set?</h2>\n",
    "\n",
    "<p> Feature scaling should be applied <b>after</b> splitting the data into training and test sets. This ensures that the test data remains unseen during training, preventing data leakage and giving a more accurate evaluation of the model’s performance. Scaling the test set separately, using the parameters (like mean and standard deviation) from the training set, helps the model generalize better to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Why feature scale the dataset?</h3>\n",
    "\n",
    "<p>Feature scaling is done to make sure all numerical features are on a similar scale, which helps machine learning models learn more effectively. Without scaling, features with larger values could dominate the model’s calculations, leading to biased results. It also speeds up training and improves accuracy, especially for models sensitive to feature magnitudes, like linear regression and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Standardisation or Normalisation?</h3>\n",
    "\n",
    "<p>Since, standardisation can be mostly applied for all types of data (normalization mostly for normal data), we prefer standardisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Do we apply Standardization to categorized columns?</h3>\n",
    "\n",
    "<p> No, because everything to make a reasonable sense of the data is already done. If we apply feature scaling to those dummy values, give nonscene data that doesn't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "# Use the same scalar used for training\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.1915918438457856 -1.0781259408412427]\n",
      " [0.0 1.0 0.0 -0.014117293757057902 -0.07013167641635401]\n",
      " [1.0 0.0 0.0 0.5667085065333239 0.6335624327104546]\n",
      " [0.0 0.0 1.0 -0.3045301939022488 -0.30786617274297895]\n",
      " [0.0 0.0 1.0 -1.901801144700799 -1.4204636155515822]\n",
      " [1.0 0.0 0.0 1.1475343068237056 1.2326533634535488]\n",
      " [0.0 1.0 0.0 1.4379472069688966 1.5749910381638883]\n",
      " [1.0 0.0 0.0 -0.7401495441200352 -0.5646194287757336]]\n",
      "[[0.0 1.0 0.0 -1.4661817944830127 -0.9069571034860731]\n",
      " [1.0 0.0 0.0 -0.44973664397484425 0.20564033932253029]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
